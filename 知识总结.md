<center><font size=9><b>RPN和RoI Pooling</b></font></center>

RPN所做的事：

1、RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。

2、Proposal Layer负责综合所有[d x(A)，d y(A)，d w(A)，d h(A)]变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer.

3、按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors。即提取修正位置后的foreground anchors.

4、将anchors映射回原图，判断fg（foreground) anchors是否大范围超过边界，剔除严重超出边界fg anchors。

5、进行nms（nonmaximum suppression，非极大值抑制）.

6、再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。

7、将最后的anchors送入ROI Pooing.

 

而RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从下图中可以看到Rol pooling层有2个输入：
 原始的feature maps

ROI Pooling的作用是通过最大池化操作将特征图上面的ROI(不是特征图）固定为特定大小的特征图（7x7），以便进行后续的分类和包围框回归操作。步骤：先将proposal映射到原图大小，再将其高宽都分成7份，对每一份都进行max pooling处理得到fixed-length output。





<center><font size=9><b>YOLOv8</b></font></center>

# 1 数据处理 [参考blog](https://blog.csdn.net/xiefanmin/article/details/135297529?ops_request_misc=&request_id=&biz_id=102&utm_term=yolov8%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E4%BB%A3%E7%A0%81&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-135297529.142^v101^pc_search_result_base2&spm=1018.2226.3001.4187)

## 1.1 数据加载

`base.py-> BaseDataset class -> load_image fun:`

1. 加载图片

2. 在保持图像长宽比不变的情况下，将图像的长边resize到self(dataset).imgsz

在load_image中，每加载一张图片，就会将该图片resize后的图存在self(dataset).ims里，以便后续数据增强模块使用。

`base.py-> BaseDataset class -> get_image_and_label fun:`
1. 调用load_image加载图片

2. 计算ratio_pad = resized_shape / ori_shape

`base.py-> BaseDataset class -> __getitem__ fun:`
1. 调用get_image_and_label加载图片和标签

2. 调用transforms: Compose进行数据增强
	transformers:
	- Compose
		- Mosaic

			当处理到第n张图时，前边已经处理了n-1张图了，所以在self(Mosaic).dataset.ims中，已经存储了n-1张图片，所以就可以从这n-1 + 1（加1是因为当前图片已经被load_image处理过了，所以当前图片也在self.dataset.ims中）张图中随机抽取若干张进行Mosaic操作。
		
		- CopyPaste
		
		- RandomPerspective
		
	- MixUp
		$$
		\lambda = Beta(\alpha, \beta) \\
		img_{mix} = img_1 * \lambda + img_2 * \lambda
		$$
		
	- Albumentations
	
	- RandomHSV
	
	- RandomFlip
	
	- RandomFlip
	
	- Format
## 1.2 数据预处理

## 1.3 数据增强

# 2 Set up train

## 2.1 Exponential Moving Average

在深度学习中，经常会使用[EMA](https://so.csdn.net/so/search?q=EMA&spm=1001.2101.3001.7020)（Exponential Moving Average）指数移动平均方法对模型参数做平均，以提高测试指标并增加模型鲁棒性。实际上，EMA可以看作是Temporal Ensembling，在模型学习过程中融合更多的历史状态，从而达到更好的优化效果。
$$
w_t = \beta * w_{t-1} + (1 - \beta) * \delta_t \\

w_t 是第t次更新得到的所有参数权重；\\
\delta_t 是第t次更新得到的所有参数移动平均数；\\
\beta 是权重参数。\\
$$

## 2.2

# 3. 优化器参数

## 3.1 learning rate

## 3.2 weight decay

## 3.3 momentum